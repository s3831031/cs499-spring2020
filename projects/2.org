K-fold cross-validation for hyper-parameter tuning and model comparison

In this project your goal is to implement K-Fold cross-validation,
then use it with machine learning algorithms to (1) train
hyper-parameters and (2) compare prediction accuracy.

For this project it is strongly encouraged to use R packages
data.table and ggplot2 for data reading and visualization, [[file:2.R][See
example/demo R script]] and additional code below:

#+BEGIN_SRC R
  ## install package if it is not already.
  if(!require("data.table")){
    install.packages("data.table")
  }

  ## attach all functions provided by these packages.
  library(data.table)
  library(ggplot2)

  ## download spam data set to local directory, if it is not present.
  if(!file.exists("spam.data")){
    download.file("https://web.stanford.edu/~hastie/ElemStatLearn/datasets/spam.data", "spam.data")
  }

  ## Read spam data set and conver to X matrix and y vector we need for
  ## gradient descent.
  spam.dt <- data.table::fread("spam.data")
  N.obs <- nrow(spam.dt)
  X.raw <- as.matrix(spam.dt[, -ncol(spam.dt), with=FALSE]) 
  y.vec <- spam.dt[[ncol(spam.dt)]]
  X.sc <- scale(X.raw) #scaled X/feature/input matrix.

  ## compute and visualize validation error as a function of number of
  ## neighbors.
  result <- NearestNeighborsCV(X.sc, y.vec)
  ggplot()+
    geom_line(aes(
      neighbors, error.percent, group=validation.fold),
      data=validation.error)

  ## compute and visualize test error results:
  err.dt.list <- list()
  ## assign folds.
  for(test.fold in 1:5){
    ## split into train/test sets.
    for(algorithm in c("baseline", "1-NN", "NNCV")){
      ## run algorithm and store test error.
      err.dt.list[[paste(test.fold, algorithm)]] <- data.table(
	test.fold, algorithm, error.percent)
    }
  }
  err.dt <- do.call(rbind, err.dt.list)

  ggplot()+
    geom_point(aes(
      error.percent, algorithm),
      data=err.dt)
#+END_SRC

*** Algorithm/function: KFoldCV

The goal of this exercise is to code the K-Fold Cross-Validation
algorithm.
- (10 points) You should code a function KFoldCV which should take as
  input arguments:
  - X_mat, a matrix of numeric inputs (one row for each observation, one column
    for each feature).
  - y_vec, a vector of binary outputs (the corresponding label for each
    observation, either 0 or 1).
  - ComputePredictions, a function that takes three inputs
    (X_train,y_train,X_new), trains a model using X_train,y_train,
    then outputs a vector of predictions (one element for every row of
    X_new).
  - fold_vec, a vector of integer fold ID numbers (from 1 to K).
- (5 points) The function should begin by initializing a variable
  called error_vec, a numeric vector of size K, to be filled with the
  mean validation error for each fold.
- (20 points) The function should have a for loop over the unique
  values k in fold_vec (should be from 1 to K). During each iteration
  k you should
  - first define X_new,y_new based on the observations for which the
    corresponding elements of fold_vec are equal to the current fold
    ID k.
  - then define X_train,y_train using all the other observations.
  - then call ComputePredictions and store the result in a variable
    named pred_new.
  - then compute the zero-one loss of pred_new with respect to y_new
    and store the mean (error rate) in the corresponding entry of
    error_vec.
- (5 points) At the end of the algorithm you should return
  error_vec.

*** Algorithm: NearestNeighborsCV

- (5 points) You should code a function NearestNeighborsCV with input
  arguments
  - X_mat, a matrix of numeric inputs/features (one row for each
    observation, one column for each feature).
  - y_vec, a vector of binary outputs (the corresponding label for each
    observation, either 0 or 1).
  - X_new, a matrix of numeric inputs/features for which we want to
    compute predictions.
  - num_folds, default value 5.
  - max_neighbors, default value 20.
- (5 points) randomly create a variable called validation_fold_vec (same number of entries as observations in y_vec/X_mat), a
  vector with integer values from 1 to num_folds. e.g. n.folds <- 5; fold.vec <- sample(rep(1:n.folds, l=nrow(X.sc)))
- (5 points) initialize a variable called error_mat, a numeric matrix
  (num_folds x max_neighbors).
- (5 points) There should be a for loop over num_neighbors from 1 to
  max_neighbors.
- (5 points) Inside the for loop you should call KFoldCV, and specify
  ComputePreditions=a function that uses your programming language's
  default implementation of the nearest neighbors algorithm, with
  num_neighbors. e.g. [[https://scikit-learn.org/stable/modules/neighbors.html][scikit-learn neighbors in Python]],
  [[https://www.rdocumentation.org/packages/class/versions/7.3-15/topics/knn][class::knn in R]]. Store the resulting error rate vector in the
  corresponding column of error_mat.
- (5 points) Compute a variable called mean_error_vec (size
  max_neighbors) by taking the mean of each column of error_mat.
- (5 points) Compute a variable called best_neighbors which is the
  number of neighbors with minimal error.
- (5 points) Your function should output (1) the predictions for X_new,
  using the entire X_mat,y_vec with best_neighbors; (2) the
  mean_error_mat for visualizing the validation error.

*** Experiments/application
- Use spam data set from
  [[https://web.stanford.edu/~hastie/ElemStatLearn/data.html]]
- First scale the inputs (each column should have mean 0 and variance
  1). You can do this by subtracting away the mean and then dividing
  by the standard deviation of each column (or just use a standard
  function like scale in R).
- (10 points) Use NearestNeighborsCV on the whole data set, then plot
  validation error as a function of the number of neighbors,
  separately for each fold.
- (10 points) Draw a bold line for the mean validation error, and draw
  a point to emphasize the minimum.
- (10 points) Randomly create a variable test_fold_vec which is a
  vector with one element for each observation, and elements are
  integers from 1 to 4. In your report please include a table of
  counts with a row for each fold (1/2/3/4) and a column for each
  class (0/1). 
- (10 points) Use KFoldCV with three algorithms: (1) baseline/underfit
  -- predict most frequent class, (2) NearestNeighborsCV, (3) overfit
  1-nearest neighbors model. Plot the resulting test error values as a
  function of the data set, in order to show that the
  NearestNeighborsCV is more accurate than the other two
  models. Example:

[[file:2-test-accuracy.png]]


*** Grading rubric (out of 120 points)

Your final grade for this project will be computed by multiplying the
percentage from your [[file:group-evals.org][group evaluations]] with your group's total score
from the rubric above.

Your group should submit a PDF on BBLearn. 
- The first thing in the PDF should be your names and student ID's
  (e.g. th798) and a link to your source code in a public repo
  (e.g. github, there should be no code in your PDF report).
- The second thing in the PDF should be your group evaluation scores
  for yourself and your teammates.

Extra credit: 
- 10 points if your github repo includes a README.org (or README.md
  etc) file with a link to the source code of your GradientDescent
  function, and an explanation about how to run it on the data sets.
- 10 points if you run your GradientDescent function through KFoldCV
  as well, and show results for that as another algorithm in your test
  error figure.
- 10 points if you compute and plot ROC curves for each (test fold,
  algorithm) combination. Make sure each algorithm is drawn in a
  different color, and there is a legend that the reader can use to
  read the figure. Example:

[[file:1-ROC.PNG]]
  
